{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld2rw4CQPfZg",
        "outputId": "1cac9c04-b5eb-4fb2-e4b1-099cd5127d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.13-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.13\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import pickle"
      ],
      "metadata": {
        "id": "Bgp7OvgsPqUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU0KoXJmPt4g",
        "outputId": "253bc234-b512-428e-b642-537ee6482efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Speech-NER/train.pkl\", 'rb') as file:\n",
        "    train = pickle.load(file)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Speech-NER/test.pkl\", 'rb') as file:\n",
        "    test = pickle.load(file)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Speech-NER/aug.pkl\", 'rb') as file:\n",
        "    aug = pickle.load(file)"
      ],
      "metadata": {
        "id": "_ynrD9kdPvR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_data = [[], []]\n",
        "for i in range(len(train[0])):\n",
        "  if train[1][i] != []:\n",
        "    seed_data[0].append(train[0][i])\n",
        "    seed_data[1].append(train[1][i])"
      ],
      "metadata": {
        "id": "6fu_Z0eRQ5B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_tags = []\n",
        "for i in seed_data[1]:\n",
        "  temp = []\n",
        "  for j in i:\n",
        "    if j['label'] not in list_of_tags:\n",
        "      temp.append(j['label'])\n",
        "  temp = list(set(sorted(temp)))\n",
        "  list_of_tags.append(tuple(temp))\n",
        "\n",
        "categorised_data = {}\n",
        "for i in list_of_tags:\n",
        "  categorised_data[i] = []"
      ],
      "metadata": {
        "id": "Hlp5i6tDYTKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_tags_with_pad(sentences, tags_list):\n",
        "    new_sentences = []\n",
        "\n",
        "    for sentence, tags in zip(sentences, tags_list):\n",
        "        sentence_chars = list(sentence)\n",
        "\n",
        "        for tag in tags[::-1]:\n",
        "            start = tag['start']\n",
        "            end = tag['end']\n",
        "            sentence_chars[start:end] = \"\"\n",
        "\n",
        "        new_sentence = ''.join(sentence_chars)\n",
        "        new_sentences.append(new_sentence)\n",
        "\n",
        "    return new_sentences\n",
        "\n",
        "struct_data = replace_tags_with_pad(seed_data[0], seed_data[1])"
      ],
      "metadata": {
        "id": "rJZKv9QSfTeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(struct_data)):\n",
        "  temp = []\n",
        "  for j in seed_data[1][i]:\n",
        "    temp.append(j['label'])\n",
        "  temp = tuple(list(set(sorted(temp))))\n",
        "  categorised_data[temp].append(struct_data[i])\n",
        "\n",
        "for i in categorised_data:\n",
        "  categorised_data[i] = list(set(categorised_data[i]))"
      ],
      "metadata": {
        "id": "RzgwCMLdpe0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_tags = {}\n",
        "for i in range(len(seed_data[1])):\n",
        "  for j in seed_data[1][i]:\n",
        "    if j['label'] not in list_of_tags:\n",
        "      list_of_tags[j['label']] = []\n",
        "    list_of_tags[j['label']].append(seed_data[0][i][j['start']:j['end']])"
      ],
      "metadata": {
        "id": "aeiAt9m6QGud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def llm_response(prompt, sentence_examples):\n",
        "#     openai = OpenAI(\n",
        "#         api_key=\"bPdeUDQV6WUDGE6ifWQHult7iii7OWFj\",\n",
        "#         base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "#     )\n",
        "\n",
        "\n",
        "#     chat_completion = openai.chat.completions.create(\n",
        "#         model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "#         messages=[\n",
        "#             {\"role\": \"system\", \"content\": f\"You are given a set of with named entity recognition (NER) tags. Your task is to generate a set of 20 new sentences that contain exactly the same NER tags as given. Each new sentence should: 1. Use exactly the same NER tags in appropriate contexts without adding any new NER tags.  2. Reflect spoken language, as if someone is speaking to another person. 3. Follow the structure as given by {sentence_examples}. The format for the input will be such that you will get the NER tags in close brackets. Given this kind of input, you need to now generate 20 more sentence that contain exactly the same NER tags as given in the prompt. DO NOT ADD BULLET NUMBERS IN THE OUTPUT.\"},\n",
        "#             {\"role\": \"user\", \"content\": prompt},\n",
        "#         ],\n",
        "#         temperature=1\n",
        "#       )\n",
        "#     return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "CpLUjsDXjw9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_response(prompt, sentence_examples):\n",
        "    openai = OpenAI(\n",
        "        api_key=\"bPdeUDQV6WUDGE6ifWQHult7iii7OWFj\",\n",
        "        base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "    )\n",
        "\n",
        "\n",
        "    chat_completion = openai.chat.completions.create(\n",
        "        model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"You are given a set of with named entity recognition (NER) tags. Your task is to generate a set of 20 new sentences that contain exactly the same NER tags as given. Each new sentence should: 1. Use exactly the same NER tags in appropriate contexts without adding any new NER tags.  2. Reflect spoken language, as if someone is speaking to another person. 3. Be as creative and diverse as you can with the structure of sentences you come up with. Talk about the NER tag in different contexts and themes.. The format for the input will be such that you will get the NER tags in close brackets. Given this kind of input, you need to now generate 20 more sentence that contain exactly the same NER tags as given in the prompt. DO NOT ADD BULLET NUMBERS IN THE OUTPUT.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=1\n",
        "      )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "598hCpOj2Vxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_to_prompt(data, tags):\n",
        "  prompt = \"\"\n",
        "  prompt = \"NER Tags = (\"\n",
        "  for tag in tags:\n",
        "    prompt += data[tag['start']:tag['end']] + \" - \" + tag['label']\n",
        "    prompt += \", \"\n",
        "  prompt = prompt + \")\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "3NxXM7GlnE5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_struct_sentences(sentence, tags):\n",
        "  struct_sentences = []\n",
        "  temp = []\n",
        "  for j in tags:\n",
        "    temp.append(j['label'])\n",
        "  temp = tuple(list(set(sorted(temp))))\n",
        "  if len(categorised_data[temp]) > 10:\n",
        "    indices = random.sample(range(0, len(categorised_data[temp])), 10)\n",
        "    for i in indices:\n",
        "      struct_sentences.append(categorised_data[temp][i])\n",
        "  else:\n",
        "    struct_sentences = categorised_data[temp]\n",
        "    for tag in temp:\n",
        "      cat_data = categorised_data[tuple(list(set(sorted([tag]))))]\n",
        "      indices = random.sample(range(0, len(cat_data)), 3)\n",
        "      for i in indices:\n",
        "        struct_sentences.append(cat_data[i])\n",
        "  return struct_sentences"
      ],
      "metadata": {
        "id": "HnrNOE8suHpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_dataset = []\n",
        "for i in tqdm(range(len(seed_data[0]))):\n",
        "  prompt = data_to_prompt(seed_data[0][i], seed_data[1][i])\n",
        "  response = llm_response(prompt, \"\")\n",
        "  augmented_dataset.append(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyOd8mm5Wfiz",
        "outputId": "0ef9280f-e4b2-4141-9e0b-59872b913b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 574/574 [1:20:03<00:00,  8.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('augmented_diverse_structure.pkl', 'wb') as f:\n",
        "  pickle.dump(augmented_dataset, f)"
      ],
      "metadata": {
        "id": "nQbX8kzmWwUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/augmented_diverse_structure.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "eI_yjd2l4UHU",
        "outputId": "e306fba7-541c-4780-d09b-7860294f9ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_27fdf2ad-d6a4-44c2-bc5a-cb902efce729\", \"augmented_diverse_structure.pkl\", 809855)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WchGVacPWX6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}