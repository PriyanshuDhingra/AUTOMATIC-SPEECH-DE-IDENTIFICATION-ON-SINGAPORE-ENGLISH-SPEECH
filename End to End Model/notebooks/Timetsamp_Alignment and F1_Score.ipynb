{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import tempfile\n",
    "from pydub import AudioSegment\n",
    "from vosk import Model, KaldiRecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'transcript_output/transcriptions_seed-gretel-similar0.3-similar0.6.jsonl'\n",
    "output_file_path = 'transcript_output/formatted_transcriptions_seed-gretel-similar0.3-similar0.6.jsonl'\n",
    "with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
    "    # Process each line in the input file\n",
    "    for line in infile:\n",
    "        # Parse the JSON object\n",
    "        data = json.loads(line)\n",
    "        # Extract the transcription field and format it as required\n",
    "        formatted_entry = {\n",
    "            \"text\": data[\"transcription\"]\n",
    "        }\n",
    "        # Write the formatted entry to the output file\n",
    "        outfile.write(json.dumps(formatted_entry) + '\\n')\n",
    "print(f\"Transformation complete. Output written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_clean_text(transcription):\n",
    "    # Define token patterns\n",
    "    patterns = {\n",
    "        '[PERSON_START]': 'PERSON',\n",
    "        '[PERSON_END]': 'PERSON',\n",
    "        '[PHONE_START]': 'PHONE',\n",
    "        '[PHONE_END]': 'PHONE',\n",
    "        '[DATE_START]': 'DATE',\n",
    "        '[DATE_END]': 'DATE',\n",
    "        '[CARDINAL_START]': 'CARDINAL',\n",
    "        '[CARDINAL_END]': 'CARDINAL',\n",
    "        '[GPE_START]': 'GPE',\n",
    "        '[GPE_END]': 'GPE',\n",
    "        '[LOC_START]': 'LOC',\n",
    "        '[LOC_END]': 'LOC',\n",
    "        '[MONEY_START]': 'MONEY',\n",
    "        '[MONEY_END]': 'MONEY',\n",
    "        '[ORG_START]': 'ORG',\n",
    "        '[ORG_END]': 'ORG',\n",
    "        '[EMAIL_START]': 'EMAIL',\n",
    "        '[EMAIL_END]': 'EMAIL',\n",
    "        '[CREDIT_CARD_START]': 'CREDIT_CARD',\n",
    "        '[CREDIT_CARD_END]': 'CREDIT_CARD',\n",
    "        '[BANK_ACCOUNT_START]': 'BANK_ACCOUNT',\n",
    "        '[BANK_ACCOUNT_END]': 'BANK_ACCOUNT',\n",
    "        '[CAR_PLATE_START]': 'CAR_PLATE',\n",
    "        '[CAR_PLATE_END]': 'CAR_PLATE',\n",
    "        '[NRIC_START]': 'NRIC',\n",
    "        '[NRIC_END]': 'NRIC',\n",
    "        '[PASSPORT_NUM_START]': 'PASSPORT_NUM',\n",
    "        '[PASSPORT_NUM_END]': 'PASSPORT_NUM',\n",
    "        '[TIME_START]': 'TIME',\n",
    "        '[TIME_END]': 'TIME'\n",
    "    }\n",
    "\n",
    "    entities = []\n",
    "    cleaned_text = []\n",
    "    current_entity = None\n",
    "    last_end = 0\n",
    "\n",
    "    # Use regex to find all special tokens\n",
    "    for match in re.finditer(r'\\[.*?\\]', transcription):\n",
    "        token = transcription[match.start():match.end()]\n",
    "        \n",
    "        # Add text between tokens to cleaned text\n",
    "        cleaned_text.append(transcription[last_end:match.start()])\n",
    "        \n",
    "        if token in patterns:\n",
    "            if token.endswith('_START]'):\n",
    "                # Start of a new entity\n",
    "                if current_entity:\n",
    "                    # Close previous entity\n",
    "                    current_entity['end'] = match.start()\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = {'start': match.start(), 'label': patterns[token]}\n",
    "            elif token.endswith('_END]'):\n",
    "                if current_entity:\n",
    "                    # End of the current entity\n",
    "                    current_entity['end'] = match.start()\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        last_end = match.end()\n",
    "    \n",
    "    # Add the remaining text after the last token\n",
    "    cleaned_text.append(transcription[last_end:])\n",
    "    \n",
    "    # Join all parts to create the cleaned transcription\n",
    "    cleaned_transcription = ''.join(cleaned_text)\n",
    "    \n",
    "    # Handle case where the last entity does not end properly\n",
    "    if current_entity:\n",
    "        current_entity['end'] = len(transcription)\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return cleaned_transcription, entities\n",
    "\n",
    "def process_file(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            transcription = data[\"text\"]\n",
    "            cleaned_text, entities = extract_entities_and_clean_text(transcription)\n",
    "\n",
    "            formatted_entry = {\n",
    "                \"text\": cleaned_text,\n",
    "                \"entities\": [[e['start'], e['end'], e['label']] for e in entities]\n",
    "            }\n",
    "            outfile.write(json.dumps(formatted_entry) + '\\n')\n",
    "\n",
    "    print(f\"Transformation complete. Output written to {output_file_path}\")\n",
    "input_file_path = 'transcript_output/formatted_transcriptions_seed-gretel-diverse0.3+0.6-similar0.3+0.6.jsonl'\n",
    "output_file_path = 'transcript_output/formatted_entities_seed-gretel-diverse0.3+0.6-similar0.3+0.6.jsonl'\n",
    "process_file(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entities(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            # Extract the \"text\" field only\n",
    "            text_entry = {\n",
    "                \"text\": data[\"text\"]\n",
    "            }\n",
    "            # Write the text-only entry to the output file\n",
    "            outfile.write(json.dumps(text_entry) + '\\n')\n",
    "\n",
    "    print(f\"Text extraction complete. Output written to {output_file_path}\")\n",
    "input_file_path = 'Pipeline_Method/Final_augmented_data_base_sim_0.3_trf.jsonl'  # Replace with your actual file\n",
    "output_file_path = 'Pipeline_Method/text_only/Final_augmented_data_base_sim_0.3_trf.jsonl'\n",
    "remove_entities(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'Pipeline_Method/text_only/Final_augmented_data_base_sim_0.3_trf.jsonl'\n",
    "output_file = 'Pipeline_Method/text_only_ids/Final_augmented_data_base_sim_0.3_trf.jsonl'\n",
    "# Open the input file and read line by line\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "    for i, line in enumerate(infile):\n",
    "        data = json.loads(line)  # Load the JSON object\n",
    "        \n",
    "        # Create an OrderedDict with 'id' as the first key\n",
    "        ordered_data = OrderedDict([('id', i + 1)])\n",
    "        ordered_data.update(data)  # Add the rest of the data\n",
    "        \n",
    "        outfile.write(json.dumps(ordered_data) + '\\n')  # Write back to the output file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_new is Vosk model\n",
    "model_path = \"model_new\"\n",
    "model = Model(model_path)\n",
    "\n",
    "def align_audio_with_text(audio_path, transcription):\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    recognizer = KaldiRecognizer(model, audio.frame_rate)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_wav:\n",
    "        temp_wav_path = temp_wav.name\n",
    "        audio.export(temp_wav_path, format=\"wav\")\n",
    "    results = []\n",
    "    try:\n",
    "        with open(temp_wav_path, \"rb\") as wf:\n",
    "            wf.read(44)\n",
    "            recognizer.SetWords(True)\n",
    "            while True:\n",
    "                data = wf.read(4000)\n",
    "                if len(data) == 0:\n",
    "                    break\n",
    "                if recognizer.AcceptWaveform(data):\n",
    "                    results.append(json.loads(recognizer.Result()))\n",
    "            results.append(json.loads(recognizer.FinalResult()))\n",
    "    finally:\n",
    "        if os.path.exists(temp_wav_path):\n",
    "            os.remove(temp_wav_path)\n",
    "\n",
    "    words = []\n",
    "    for result in results:\n",
    "        if 'result' in result:\n",
    "            for word in result['result']:\n",
    "                words.append(word)\n",
    "    \n",
    "    aligned_segments = []\n",
    "    for word in words:\n",
    "        aligned_segments.append({\n",
    "            \"start\": word[\"start\"],\n",
    "            \"end\": word[\"end\"],\n",
    "            \"word\": word[\"word\"]\n",
    "        })\n",
    "\n",
    "    return aligned_segments\n",
    "audio_dir = \"Audio_Files_for_testing\"\n",
    "transcription_file = \"Pipeline_Method/text_only_ids/Final_augmented_data_base_sim_0.3_trf.jsonl\"\n",
    "output_file = \"Pipeline_Method/aligned_output/Final_augmented_data_base_sim_0.3_trf.jsonl\"\n",
    "with open(transcription_file, 'r') as f:\n",
    "    transcriptions = [json.loads(line) for line in f]\n",
    "aligned_data = []\n",
    "for item in transcriptions:\n",
    "    audio_path = f\"{audio_dir}/id{item['id']}.wav\"\n",
    "    aligned_transcription = align_audio_with_text(audio_path, item['text'])\n",
    "    aligned_data.append({\n",
    "        \"id\": item['id'],\n",
    "        \"text\": item['text'],\n",
    "        \"align\": aligned_transcription\n",
    "    })\n",
    "with open(output_file, 'w') as f:\n",
    "    for item in aligned_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Merging of jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Pipeline_Method/aligned_output/Final_augmented_data_base_sim_0.3_trf.jsonl', 'r') as f:\n",
    "    aligned_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Load the filtered test data\n",
    "with open('Pipeline_Method/Final_augmented_data_base_sim_0.3_trf.jsonl', 'r') as f:\n",
    "    filtered_test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Create a mapping from text to entities for easy lookup\n",
    "filtered_test_map = {item['text']: item['entities'] for item in filtered_test_data}\n",
    "\n",
    "# Merge entities from the second file into the first\n",
    "for item in aligned_data:\n",
    "    text = item['text']\n",
    "    if text in filtered_test_map:\n",
    "        item['entities'] = filtered_test_map[text]\n",
    "    else:\n",
    "        item['entities'] = []  # Or handle cases where no entities are found, if needed\n",
    "with open('Pipeline_Method/merged_output/Final_augmented_data_base_sim_0.3_trf.jsonl', 'w') as f:\n",
    "    for item in aligned_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex patterns for different entities\n",
    "patterns = {\n",
    "    'CREDIT_CARD': r'\\d{4}-\\d{4}-\\d{4}-\\d{4}',\n",
    "    'BANK_ACCOUNT': r'\\d{3}-\\d{5}-\\d',\n",
    "    'PASSPORT_NUM': r'K\\d{7}[A-Z]',\n",
    "    'PHONE': r'\\d{4}-\\d{4}',\n",
    "    'NRIC': r'[STFG]\\d{7}[A-Z]'\n",
    "}\n",
    "\n",
    "def extract_entity_times(text, entities, alignments):\n",
    "    timestamped_entities = []\n",
    "    alignment_dict = {}\n",
    "    for alignment in alignments:\n",
    "        normalized_word = alignment['word'].lower()\n",
    "        alignment_dict[normalized_word] = {\n",
    "            'start': alignment['start'],\n",
    "            'end': alignment['end']\n",
    "        }\n",
    "\n",
    "    # Function to convert digit and hyphen sequences\n",
    "    def convert_digits_and_hyphens(entity_text):\n",
    "        replacements = {\n",
    "            '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
    "            '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine',\n",
    "            '-': ' '\n",
    "        }\n",
    "        converted_text = []\n",
    "        for char in entity_text:\n",
    "            if char in replacements:\n",
    "                converted_text.append(replacements[char])\n",
    "            else:\n",
    "                converted_text.append(char)\n",
    "        return ' '.join(converted_text).split()\n",
    "    \n",
    "    for entity in entities:\n",
    "        start_token, end_token, label = entity\n",
    "        if start_token < 0 or start_token >= len(text):\n",
    "            print(f\"Warning: Start token index out of range for entity: {entity}\")\n",
    "            continue\n",
    "        if end_token < start_token:\n",
    "            print(f\"Warning: End token index is before start token for entity: {entity}\")\n",
    "            end_token = start_token\n",
    "        if end_token >= len(text):\n",
    "            end_token = len(text) - 1\n",
    "        # Extract and normalize the text for the entity\n",
    "        entity_text = text[start_token:end_token + 1].strip()\n",
    "        if not entity_text:\n",
    "            print(f\"Warning: Extracted entity text is empty for entity: {entity}\")\n",
    "            continue\n",
    "        normalized_entity_text = entity_text.lower()        \n",
    "        # Handle numeric digit sequences specially\n",
    "        if normalized_entity_text[0].isdigit() and normalized_entity_text[-1].isdigit():\n",
    "            normalized_entity_text = convert_digits_and_hyphens(normalized_entity_text)\n",
    "            start_time = alignment_dict.get(normalized_entity_text[0], {}).get('start')\n",
    "            end_time = alignment_dict.get(normalized_entity_text[-1], {}).get('end')\n",
    "            if start_time is None or end_time is None:\n",
    "                print(f\"Error: Could not find alignment for entity: {entity}\")\n",
    "                continue\n",
    "            timestamped_entities.append([start_time, end_time, label])\n",
    "            continue\n",
    "        # Check if the entity label has a predefined pattern\n",
    "        if label in patterns:\n",
    "            pattern = patterns[label]\n",
    "            # Search for the entity text pattern in the alignments\n",
    "            pattern_str = ' '.join([a['word'] for a in alignments])\n",
    "            match = re.search(pattern, pattern_str)\n",
    "            if match:\n",
    "                # Extract the match start and end positions from alignments\n",
    "                matched_text = match.group(0)\n",
    "                start_time = None\n",
    "                end_time = None\n",
    "                current_text = \"\"\n",
    "                for alignment in alignments:\n",
    "                    word = alignment['word'].lower()\n",
    "                    if matched_text.startswith(current_text + word):\n",
    "                        if start_time is None:\n",
    "                            start_time = alignment['start']\n",
    "                        current_text += word\n",
    "                        end_time = alignment['end']\n",
    "                        if matched_text == current_text:\n",
    "                            break\n",
    "                if start_time is None or end_time is None:\n",
    "                    print(f\"Warning: Could not find alignment for entity: {entity}\")\n",
    "                    continue\n",
    "                timestamped_entities.append([\n",
    "                    start_time,\n",
    "                    end_time,\n",
    "                    label\n",
    "                ])\n",
    "                continue\n",
    "\n",
    "        # If not using regex patterns, split entity text into words\n",
    "        words_in_entity = normalized_entity_text.split()\n",
    "        start_time = None\n",
    "        end_time = None\n",
    "\n",
    "        # Track the matched words and their timestamps\n",
    "        current_text = \"\"\n",
    "        for word in words_in_entity:\n",
    "            if word in alignment_dict:\n",
    "                if start_time is None:\n",
    "                    start_time = alignment_dict[word]['start']\n",
    "                end_time = alignment_dict[word]['end']\n",
    "            else:\n",
    "                print(f\"Warning: Word '{word}' from entity text not found in alignments.\")\n",
    "\n",
    "        # If start or end time is not found, log a warning and continue\n",
    "        if start_time is None or end_time is None:\n",
    "            print(f\"Warning: Could not find alignment for entity: {entity}\")\n",
    "            continue\n",
    "\n",
    "        # Append the entity with its start time, end time, and label\n",
    "        timestamped_entities.append([\n",
    "            start_time,\n",
    "            end_time,\n",
    "            label\n",
    "        ])\n",
    "\n",
    "    return timestamped_entities\n",
    "\n",
    "def process_jsonl(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            item = json.loads(line.strip())\n",
    "            text = item['text']\n",
    "            alignments = item['align']\n",
    "            entities = item['entities']\n",
    "            # Extract the timestamped entities\n",
    "            timestamped_entities = extract_entity_times(text, entities, alignments)\n",
    "            # Create the new json structure\n",
    "            output_data = {\n",
    "                \"text\": text,\n",
    "                \"entities\": timestamped_entities\n",
    "            }\n",
    "            outfile.write(json.dumps(output_data) + '\\n')\n",
    "\n",
    "# Paths to your input and output files\n",
    "input_file = 'transcript_output/merged_output/merged_data_seed_gretel_similar0.3.jsonl'\n",
    "output_file = 'transcript_output/timestamped/processed_data_seed_gretel_similar0.3.jsonl'\n",
    "\n",
    "# Process the file\n",
    "process_jsonl(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the overlap function\n",
    "def calculate_overlap(start1, end1, start2, end2, threshold):\n",
    "    overlap_start = max(start1, start2)\n",
    "    overlap_end = min(end1, end2)\n",
    "    overlap_duration = max(0, overlap_end - overlap_start)\n",
    "    duration1 = end1 - start1\n",
    "    duration2 = end2 - start2\n",
    "    overlap_ratio = overlap_duration / min(duration1, duration2)\n",
    "    return overlap_ratio >= threshold\n",
    "\n",
    "# Define the F1 score calculation function\n",
    "def calculate_f1_scores(true_entities, predicted_entities, overlap_threshold, target_labels):\n",
    "    true_dict = {}\n",
    "    predicted_dict = {}\n",
    "    \n",
    "    # Create dictionaries of lists for true and predicted entities\n",
    "    for ent in true_entities:\n",
    "        label = ent[2]\n",
    "        start, end = ent[0], ent[1]\n",
    "        if label in target_labels:\n",
    "            if label not in true_dict:\n",
    "                true_dict[label] = []\n",
    "            true_dict[label].append((start, end))\n",
    "\n",
    "    for ent in predicted_entities:\n",
    "        label = ent[2]\n",
    "        start, end = ent[0], ent[1]\n",
    "        if label in target_labels:\n",
    "            if label not in predicted_dict:\n",
    "                predicted_dict[label] = []\n",
    "            predicted_dict[label].append((start, end))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for label in target_labels:\n",
    "        true_intervals = true_dict.get(label, [])\n",
    "        pred_intervals = predicted_dict.get(label, [])\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for start_true, end_true in true_intervals:\n",
    "            matched = False\n",
    "            for start_pred, end_pred in pred_intervals:\n",
    "                if calculate_overlap(start_true, end_true, start_pred, end_pred, overlap_threshold):\n",
    "                    matched = True\n",
    "                    break\n",
    "            y_true.append(1)\n",
    "            y_pred.append(1 if matched else 0)\n",
    "        \n",
    "        for start_pred, end_pred in pred_intervals:\n",
    "            if not any(calculate_overlap(start_pred, end_pred, start_true, end_true, overlap_threshold) \n",
    "                       for start_true, end_true in true_intervals):\n",
    "                y_true.append(0)\n",
    "                y_pred.append(1)\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        results.append({\n",
    "            'Label': label,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "        \n",
    "        # Collect all true and predicted labels for overall metrics\n",
    "        all_true_labels.extend(y_true)\n",
    "        all_pred_labels.extend(y_pred)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n",
    "        all_true_labels, all_pred_labels, average='weighted', zero_division=0)\n",
    "    \n",
    "    results.append({\n",
    "        'Label': 'Overall',\n",
    "        'Precision': overall_precision,\n",
    "        'Recall': overall_recall,\n",
    "        'F1 Score': overall_f1\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load data from JSONL files\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = [json.loads(line.strip()) for line in file]\n",
    "    return data\n",
    "\n",
    "# Set file paths and overlap threshold\n",
    "true_file = 'transcript_output/timestamped/processed_data_true.jsonl'\n",
    "pred_file = 'transcript_output/timestamped/processed_data_seed_gretel_similar0.3.jsonl'\n",
    "overlap_threshold = 1.0 # 20% overlap\n",
    "\n",
    "# Define target labels\n",
    "target_labels = ['EMAIL', 'NRIC', 'CREDIT_CARD','PHONE','PERSON','PASSPORT_NUM', 'BANK_ACCOUNT', 'CAR_PLATE']\n",
    "\n",
    "# Load the data\n",
    "true_data = load_data(true_file)\n",
    "pred_data = load_data(pred_file)\n",
    "\n",
    "# Extract entities\n",
    "true_entities = [item for d in true_data for item in d['entities']]\n",
    "predicted_entities = [item for d in pred_data for item in d['entities']]\n",
    "\n",
    "# Calculate F1 scores\n",
    "results = calculate_f1_scores(true_entities, predicted_entities, overlap_threshold, target_labels)\n",
    "\n",
    "# Create DataFrame and display results\n",
    "df = pd.DataFrame(results)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
